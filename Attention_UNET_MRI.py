# -*- coding: utf-8 -*-
"""dl-project-attention-unet-pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BoX0TpKUU_QAxhjeIlVb88WhM0ZFjnrq
"""

!pip install albumentations==0.4.6

import numpy as np
import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
import time
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2, ToTensor
from sklearn.model_selection import train_test_split

plt.style.use("dark_background")

from google.colab import drive
drive.mount('/content/drive')

! pip install -q kaggle
! mkdir ~/.kaggle
! cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation

!unzip /content/lgg-mri-segmentation.zip

dataset_base_dir= "/content/lgg-mri-segmentation/kaggle_3m"

"""# Get & Prepare Data"""

def get_images_mask_filepath(root_dir):
  image_data = []

  for patient_dir in os.listdir(root_dir):
      dir_path = os.path.join(root_dir, patient_dir)
      if os.path.isdir(dir_path):
          for filename in os.listdir(dir_path):
              img_path = os.path.join(dir_path, filename)
              image_data.append([patient_dir, img_path])
      
  #making the data frame of image paths        
  df = pd.DataFrame(image_data, columns=["dir_name", "image_path"])

  #segregating dataframe into data frame containing image directories
  #mask image ends with masks
  
  df_imgs = df[~df["image_path"].str.contains("mask")]
  df_masks = df[df["image_path"].str.contains("mask")]

  return df_imgs,df_masks

def get_data_frame(df_imgs,df_mask,L):
  end_len = 4  #define the length of .tif
  end_mask_len = 9 #define the length of _mask.tif
  imgs = sorted(df_imgs["image_path"].values, key= lambda x: int(x[L: -end_len]))
  masks = sorted(df_masks["image_path"].values, key=lambda x: int(x[L: -end_mask_len]))
  df = pd.DataFrame({"patient": df_imgs.dir_name.values,"image_path": imgs,"mask_path": masks})
  return df

df_imgs,df_masks = get_images_mask_filepath(dataset_base_dir)

df_imgs.iloc[0][1]

#identifying the index at which the image ID begins
x = len("/content/lgg-mri-segmentation/kaggle_3m/TCGA_HT_8106_19970727/TCGA_HT_8106_19970727_8")
print(x)

df = get_data_frame(df_imgs,df_masks,x-1)
df.head()

def add_diagnosis(mask_path):
    val = np.max(cv2.imread(mask_path))
    if val > 0: return 1
    else: return 0

df["diagnosis"] = df["mask_path"].apply(lambda x: add_diagnosis(x))
df.head()

df.shape

print("Amount of patients: ", len(set(df.patient)))
print("Amount of records: ", len(df))

"""# Data Augmentation"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

img_size = 128

transforms = A.Compose([
    A.Resize(width = img_size, height = img_size, p=1.0),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.Transpose(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),

    A.Normalize(p=1.0),
    ToTensor(),
    
])

class BrainMRIDataset:
    def __init__(self, df, transforms=None):
        self.df = df
        self.transforms = transforms
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        image = cv2.imread(self.df.iloc[idx, 1])
        mask = cv2.imread(self.df.iloc[idx, 2], 0)
        
        augmented = self.transforms(image=image,mask=mask)
        
        if self.transforms is not None:
          image = augmented["image"]
          mask = augmented["mask"]
        
        return image, mask

"""# Split Data and DataLoaders"""

def get_data_loader(df,batch_size,transforms=None):
  train_df, val_df = train_test_split(df, stratify=df.diagnosis, test_size=0.1)
  train_df = train_df.reset_index(drop=True)
  val_df = val_df.reset_index(drop=True)

  train_df, test_df = train_test_split(train_df, stratify=train_df.diagnosis, test_size=0.12)
  train_df = train_df.reset_index(drop=True)

  print(f"Train_dataframe: {train_df.shape} \nVal_dataframe: {val_df.shape} \nTest_dataframe: {test_df.shape}")
  train_dataset = BrainMRIDataset(train_df, transforms=transforms)
  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True)

  val_dataset = BrainMRIDataset(val_df, transforms=transforms)
  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2, shuffle=True)

  test_dataset = BrainMRIDataset(test_df, transforms=transforms)
  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2, shuffle=True)

  return train_dataloader, val_dataloader, test_dataloader

train_dataloader, val_dataloader, test_dataloader = get_data_loader(df,32,transforms)

def show_images(imgs, norm=False):
    plt.figure(figsize=(8, 8))
    nrows=5 
    ncols=5
    plt.subplots_adjust(wspace=0., hspace=0.)
    i_ = 0
    if len(imgs) > 25:
        imgs = imgs[:25]
        
    for idx in range(len(imgs)):
    
        # normalization
        if norm:           
            img = imgs[idx].numpy().transpose(1,2,0)
            mean = [0.485, 0.456, 0.406]
            std = [0.229, 0.224, 0.225] 
            img = (img*std+mean).astype(np.float32)
            
        else:
            img = imgs[idx].numpy().astype(np.float32)
            img = img[0,:,:]
        
        plt.subplot(nrows, ncols, i_+1)
        plt.imshow(img); 
        plt.axis('off')
 
        i_ += 1
        
    return plt.show()

images, masks = next(iter(train_dataloader))
print(images.shape, masks.shape)

show_images(images)
show_images(masks, norm=False)

"""# Attention U-Net

## Model Blocks
"""

#defining the convolution operations
class Conv_Block(nn.Module):
    def __init__(self, ch_in, ch_out):
        super(Conv_Block,self).__init__()
        self.conv_block = nn.Sequential(
                                  nn.Conv2d(ch_in, ch_out,kernel_size=3, stride=1,padding=1, bias=True),
                                  nn.BatchNorm2d(ch_out),
                                  nn.ReLU(inplace=True),
                                  nn.Conv2d(ch_out, ch_out,kernel_size=3, stride=1,padding=1, bias=True),
                                  nn.BatchNorm2d(ch_out),
                                  nn.ReLU(inplace=True),
        )
        
    def forward(self, x):
        x = self.conv_block(x)
        return x

#defining the up convolution for decoder path
class Up_Conv_Block(nn.Module):
    def __init__(self, ch_in, ch_out):
        super(Up_Conv_Block,self).__init__()
        self.up_sample = nn.Sequential(
                                nn.Upsample(scale_factor=2),nn.Conv2d(ch_in, ch_out,kernel_size=3,stride=1,padding=1, bias=True),
                                nn.BatchNorm2d(ch_out),
                                nn.ReLU(inplace=True),
        )
        
    def forward(self, x):
        x = x = self.up_sample(x)
        return x

class Attention_Block(nn.Module):

    def __init__(self, filter_next, filter_present, f_int):
        super(Attention_Block,self).__init__()
        #making the filters of equal size in upsampled layer and present layer
        #upsampled layer
        self.W1= nn.Sequential(nn.Conv2d(filter_next, f_int,kernel_size=1, stride=1,padding=0, bias=True),
                                nn.BatchNorm2d(f_int)
        )

        #previous layer
        self.W2 = nn.Sequential(nn.Conv2d(filter_present, f_int,kernel_size=1, stride=1,padding=0, bias=True),
                                nn.BatchNorm2d(f_int)
        )
        
        self.weights = nn.Sequential(nn.Conv2d(f_int, 1,kernel_size=1, stride=1,padding=0,  bias=True),
                                nn.BatchNorm2d(1),
                                nn.Sigmoid(),
        )
        
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self, g, x):
        #getting same number filters
        g1 = self.W1(g)
        x1 = self.W2(x)
        x2 = self.relu(g1+x1)

        #getting the weights
        weights = self.weights(x2)
        
        return weights*x

"""## Model"""

class Attention_UNet(nn.Module):
    def __init__(self, n_classes=1, in_channel=3, out_channel=1):
        super().__init__() 
        
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        self.conv1 = Conv_Block(ch_in=in_channel, ch_out=64)
        self.conv2 = Conv_Block(ch_in=64, ch_out=128)
        self.conv3 = Conv_Block(ch_in=128, ch_out=256)
        self.conv4 = Conv_Block(ch_in=256, ch_out=512)
        self.conv5 = Conv_Block(ch_in=512, ch_out=1024)
        
        self.up5 = Up_Conv_Block(ch_in=1024, ch_out=512)
        self.att5 = Attention_Block(512, 512, 256)
        self.upconv5 = Conv_Block(ch_in=1024, ch_out=512)
        
        self.up4 = Up_Conv_Block(ch_in=512, ch_out=256)
        self.att4 = Attention_Block(256, 256, 128)
        self.upconv4 = Conv_Block(ch_in=512, ch_out=256)
        
        self.up3 = Up_Conv_Block(ch_in=256, ch_out=128)
        self.att3 = Attention_Block(128, 128, 64)
        self.upconv3 = Conv_Block(ch_in=256, ch_out=128)
        
        self.up2 = Up_Conv_Block(ch_in=128, ch_out=64)
        self.att2 = Attention_Block(64, 64, 32)
        self.upconv2 = Conv_Block(ch_in=128, ch_out=64)
        
        self.conv_1x1 = nn.Conv2d(64, out_channel,
                                  kernel_size=1, stride=1, padding=0)
        
    def forward(self, x):
        # encoder
        x1 = self.conv1(x)
        
        x2 = self.maxpool(x1)
        x2 = self.conv2(x2)
        
        x3 = self.maxpool(x2)
        x3 = self.conv3(x3)
        
        x4 = self.maxpool(x3)
        x4 = self.conv4(x4)
        
        x5 = self.maxpool(x4)
        x5 = self.conv5(x5)
        
        # decoder + concat
        d5 = self.up5(x5)
        x4 = self.att5(g=d5, x=x4)
        d5 = torch.concat((x4, d5), dim=1)
        d5 = self.upconv5(d5)
        
        d4 = self.up4(d5)
        x3 = self.att4(g=d4, x=x3)
        d4 = torch.concat((x3, d4), dim=1)
        d4 = self.upconv4(d4)
        
        d3 = self.up3(d4)
        x2 = self.att3(g=d3, x=x2)
        d3 = torch.concat((x2, d3), dim=1)
        d3 = self.upconv3(d3)
        
        d2 = self.up2(d3)
        x1 = self.att2(g=d2, x=x1)
        d2 = torch.concat((x1, d2), dim=1)
        d2 = self.upconv2(d2)
        
        d1 = self.conv_1x1(d2)
        
        return d1

attention_unet = Attention_UNet(n_classes=1).to(device)

"""# Segmentation Metric"""

def dice_coef_metric(preds, target):
    if target.sum() == 0 and preds.sum() == 0:
        return 1.0 
    intersection = 2.0 * (target*preds).sum()
    union = target.sum() + preds.sum()
    
    return intersection/union

"""# Segmentation Loss"""

class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        inputs = F.sigmoid(inputs)     
        intersection = (inputs.view(-1)* targets.view(-1)).sum()                            
        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        
        return 1 - dice

"""# Training"""

def evaluate_model(model, loader, threshold=0.3):
    val_loss = 0
    
    with torch.no_grad():

        for batch_idx, batch in enumerate(loader):
            data , target = batch
            data = data.to(device)
            target = target.to(device)
            
            outputs = model(data)

            out_bin = np.copy(outputs.data.cpu().numpy())
            out_bin[np.nonzero(out_bin < threshold)] = 0.0
            out_bin[np.nonzero(out_bin >= threshold)] = 1.0
            img_loss = dice_coef_metric(out_bin, target.data.cpu().numpy())
            val_loss += img_loss

    return val_loss / batch_idx

def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs):
    
    
    loss_epoch,train_epoch,val_epoch = [],[],[]

    
    for epoch in range(num_epochs):
        model.train()
        
        losses_imgs = []
        train_iou_imgs = []
        
        for batch_idx, batch in enumerate(tqdm(train_loader)):
            data , target = batch
            data = data.to(device)
            target = target.to(device)
            
            outputs = model(data)
            
            out_bin = np.copy(outputs.data.cpu().numpy())
            out_bin[np.nonzero(out_bin < 0.5)] = 0.0
            out_bin[np.nonzero(out_bin >= 0.5)] = 1.0
            
            train_dice = dice_coef_metric(out_bin, target.data.cpu().numpy())
            
            loss = train_loss(outputs, target)
            
            losses_imgs.append(loss.item())
            train_iou_imgs.append(train_dice)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
        val_mean_iou = evaluate_model(model, val_loader)
        
        loss_epoch.append(np.array(losses_imgs).mean())
        train_epoch.append(np.array(train_iou_imgs).mean())
        val_epoch.append(val_mean_iou)
        
        print("Epoch [%d] : Batch [%d]" % (epoch, batch_idx))
        print("Loss train:[%d], DICE train:[%d], DICE validation:[%d]" % (np.array(losses_imgs).mean(), np.array(train_iou_imgs).mean(), val_mean_iou))

        
    return loss_epoch, train_epoch, val_epoch

opt = torch.optim.Adamax(attention_unet.parameters(), lr=1e-3)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# num_epochs = 50
# # after 30 does not improve much
# loss_history, train_history, val_history = train_model("Attention UNet", attention_unet, train_dataloader, val_dataloader, DiceLoss(), opt, False, num_epochs)

X = list(np.arange(num_epochs))


df = pd.DataFrame({
    'Epochs': X,
    'Training_Loss': loss_history,
    'IoU Training': train_history,
    'IoU Validation': val_history})

df.tail()

import pandas as pd


# save the DataFrame to a CSV file
df.to_csv('/content/drive/MyDrive/DL_Assign/DL_Assignb4/training_accuracy_loss.csv')

# # save the DataFrame to an Excel file
df.to_excel('/content/drive/MyDrive/DL_Assign/DL_Assignb4/training_accuracy_loss.xlsx')

x = np.arange(num_epochs)

fig = plt.figure(figsize=(10, 6))
plt.plot(x, train_history, label='train dice', lw=3, c="springgreen")
plt.plot(x, val_history, label='validation dice', lw=3, c="deeppink")

plt.title("Accuracy analysis", fontsize=15)
plt.legend(fontsize=12)
plt.xlabel("Epoch", fontsize=15)
plt.ylabel("DICE", fontsize=15)

fn = str(int(time.time())) + ".png"
plt.show()

# plot_model_history("Attention U-Net", aun_th, aun_vh, num_ep)

x = np.arange(num_epochs)

fig = plt.figure(figsize=(10, 6))
plt.plot(x, loss_history, lw=3, c="springgreen")
plt.xlabel("Epoch", fontsize=15)
plt.ylabel("DICE", fontsize=15)
plt.title("Loss analysis", fontsize=15)

test_iou = evaluate_model(attention_unet, test_dataloader)
print(test_iou)

import torch

# save the model to a file
torch.save(attention_unet.state_dict(), '/content/drive/MyDrive/DL_Assign/DL_attention_unet_1.pt')

import torch

# define the path to the saved model file
model_path = '/content/drive/MyDrive/DL_Assign/DL_attention_unet_1.pt'

# create an instance of your model

model=Attention_UNet(n_classes=1).to(device)
# load the saved state dictionary into the model
state_dict = torch.load(model_path)
model.load_state_dict(state_dict)

# set the model to evaluation mode
test_iou = evaluate_model(model, test_dataloader)
print(test_iou)



import random
count=0
for i, (images, masks) in enumerate(test_dataloader):
  if count == 3:
    break
  images = images.to(device)
  masks = masks.to(device)
  preds = model(images)
  preds = preds.detach().cpu().numpy()
  images = images.detach().cpu().numpy()
  masks = masks.detach().cpu().numpy()

  j = random.randint(0, 31)
  image = images[j]
  mask = masks[j]
  prediction = preds[j]

  fig, ax = plt.subplots(1, 3, figsize=(15, 5))

  # plot the original image
  ax[0].imshow(image.transpose(1, 2, 0))
  ax[0].set_title('Original Image')

  #plot the ground truth mask
  ax[1].imshow(mask.squeeze(), cmap='gray')
  ax[1].set_title('Ground Truth Mask')

  # plot the predicted mask
  ax[2].imshow(prediction.squeeze(), cmap='gray')
  ax[2].set_title('Predicted Mask')
  count=count+1